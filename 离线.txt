1.Nginx反向代理，将前端埋点数据、日志数据、以及广告曝光点击检测数据转发到日志服务器，通过Flume
TailDir Source监控文件，采集到Kafka
2.Maxwell监听Binlog，同步增量数据到Kafka，DataX同步业务数据服务器、广告管理平台数据库，每日将数据全
量同步到Kafka3.Flume自定义过滤器，对用户行为数据解决用户行为数据零点漂移问题，对业务数据处理时间戳精度问题，并添
加表名字段，对广告日志数据去掉各字段引号，添加时间戳字段
4.消费Kafka主题，将数据按大小、时间分批，使用Lzo压缩格式，按照日期分区，发送到HDFS，做为原始数据的
备份与展示
5.从业务数据中过滤出维度数据，尽可能丰富维度，对数据进行脱敏，进行维度整合，针对缓慢变化维，设计拉链
表，存入Hive
6.过滤事实数据，尽可能最细粒度，进行维度退化，针对存量型指标，设计周期型快照事实表
7.针对多事实指标，设计累积型快照事实表，并对join产生的数据倾斜问题进行优化
8.针对用户行为数据，对业务过程字段使用炸裂函数进行拆分过滤，存入Hive
9.从广告日志数据解析出异常流量，使用正则匹配出ip和ua，调用ip2region进行ip解析，调用HuTool对ua进行解
析，根据需求定义无效流量，并进行维度退化，存入Doris，后期使用FineBI进行可视化
10.使用Hive On Spark模式，从指标体系中抽取相同的业务过程，按照1n，dn，tn周期进行聚合，对group by产
生的数据倾斜问题进行优化，聚合后的数据进行维表关联
11.将宽表数据进行二次聚合或计算，得到可应用的数据，提供给FineBI完成各种可视化
12.使用DolphinScheduler，配置预警，编写工作流调度Shell脚本，完成数据全量增量导入，各层数据装载

1.各渠道流量统计 2.流量分时统计 3.新老访客流量统计
4.关键词统计 5.用户变动统计 6.用户新增活跃统计
7.用户行为漏斗分析 8.新增交易用户统计 9.交易综合统计